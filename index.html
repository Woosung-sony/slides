<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Woosung Choi - Dissertation</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section data-markdown data-separator="---$" data-separator-vertical="--">
					<script type="text/template">
						## Deep Learning-based Latent Source Analysis for Source-aware Audio Manipulation

						Woosung Choi (ws_choi@korea.ac.kr)
		
						--
		
						#### Deep Learning-based
						#### Latent Source Analysis 
						#### for Source-aware Audio Manipulation
		
						Woosung Choi (ws_choi@korea.ac.kr)
		
						Ph.D. Candidate, Department of Computer Science
		
						College of Informatics, Korea University
		
						June 01, 2021
		
						--
		
						Supervisor: Prof. Soonyoung Jung

						--

						## Opening BGM
						<audio controls src="assets/www_vocals.mp3" data-autoplay>Opening bgm</audio>

						--
						## Audio Manipulation on Specified Sources!
						
						- What a wonderful world - Louis Armstrong
						
						<iframe width="560" height="315" src="https://www.youtube.com/embed/p42esUHqq8Q?start=16" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

						```python
						model.manipulate_track(audio.T, "remove vocals") 
						```
					</script>
				</section>

				<section data-markdown data-separator="---$" data-separator-vertical="--">
					<script type="text/template">
						## Contents
						- Introduction

						- Part 1: FTB for Source Separaton
						- Part 2: Latent Source Analysis for Source Separation
						- Part 3: Beyond Source Separation: AMSS!

						- Discussion and Conclusion

						---

						### Introduction

						- It is difficult for non-experts to edit multimedia content such as image, audio, and video.

						![](assets/image_editing.jpg)

						--

						### Object Removal with Photoshop

						![https://digitalsynopsis.com/design/photoshop-remove-unwanted-objects-content-aware-fill/](assets/photoshop.jpg)

						https://digitalsynopsis.com/design/photoshop-remove-unwanted-objects-content-aware-fill/

						--

						### Image Inpainting - Nvidia

						- When deep learning meets image editing

						![](assets/nvidia.gif) 

						https://www.nvidia.com/en-us/research/ai-playground/

						--
						
						### Image Inpainting - Nvidia

						- When deep learning meets image editing

						![](assets/nvidia2.gif)

						https://www.nvidia.com/en-us/research/ai-playground/
										
						--
						
						### Describe What To Change!

						<img src=assets/dwtc.png width=70%/>

						Liu, Yahui, et al. "**Describe What to Change**: A Text-guided Unsupervised Image-to-Image Translation Approach." Proceedings of the 28th ACM International Conference on Multimedia. 2020.

						--

						### AI-based Image Editing Tools!
						![](assets/market.png)

						--

						### Meanwhile, Interfaces for Audio ...

						<img src="https://camo.githubusercontent.com/ef1cae7eecfd776267c33da5a314a1d3c5b30a810a62aa855c86523b41ff2f1f/68747470733a2f2f7777772e6e61746976652d696e737472756d656e74732e636f6d2f7479706f3374656d702f706963732f696d672d63652d696e74726f5f7061726167726170685f666163656c6966745f6d6173736976652d38313839323733613135313539363865666632633362656239653261616339632d6d4032782e6a7067" width=60%/>

						It's massive!

						--

						### Audio Editing	

						- In general, we want to edit specific objects in the given audio
							- e.g., I want to decrease the volume of drums 
							- (but don't want to mute them)
						- However, multitrack is usually not observable for non-experts

						--

						### Audio Editing is more challenging 

						![](assets/challenge.png)

						- A sound object (i.e., a waveform sample or frequency bin) is `transparent'
						- It usually carries information from multiple sources, in contrast to a pixel in an image

						--

						### The goal of this dissertation
						<p class="fragment" data-fragment-index="2">To design a neural network </p> 
						<p class="fragment" data-fragment-index="3">that performs audio transformations to user-specified sources (e.g., vocals) of a given audio track </p>
						<p class="fragment" data-fragment-index="4">according to a given description while preserving other sources not mentioned in the description</p>
						<br>
						<p class="fragment" data-fragment-index="5">, also known as AMSS!</p>
					
						--
						<center>
						<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Audio source separation is boring. AMSS is the new cool thing now! Next step I guess is stg like Siri integration into Logic, &quot;Siri, a bit more warmth to vocals, and make the guitar tone more brown. Please.&quot; <a href="https://twitter.com/hashtag/sourceseparation?src=hash&amp;ref_src=twsrc%5Etfw">#sourceseparation</a> <a href="https://twitter.com/hashtag/audioseparation?src=hash&amp;ref_src=twsrc%5Etfw">#audioseparation</a> <a href="https://t.co/VJcKT3JtY7">https://t.co/VJcKT3JtY7</a></p>&mdash; András Barják (@Forevian) <a href="https://twitter.com/Forevian/status/1391492024583561218?ref_src=twsrc%5Etfw">May 9, 2021</a></blockquote> 
						</center>
						--

						### Terminologies
						<p class="fragment" data-fragment-index="2"><b>audio signal</b>: a series of sampled values</p> 
						<p class="fragment" data-fragment-index="3"><b>frequency</b>: the number of occurrences of a repeating event per unit of time </p>
						<p class="fragment" data-fragment-index="4"><b>timbre</b>: perceived sound quality of a musical note, </p>
						<p class="fragment" data-fragment-index="5">, which makes a particular musical instrument or human voice have a different sound from anothe </p>

					</script>

				</section>

				<section data-markdown data-separator="---$" data-separator-vertical="--">
					<script type="text/template">
						### Part 1: FTB for Source Separation

						--

						### Part 1: FTB for Source Separation
						
						1. **review**: a U-Net for Spectrogram-based Source Separation
						2. **motivation**: Spectrogram $\neq$ Image
							 - What's wrong with CNNs and spectrograms for audio processing?
							 - Alternatives: 1-D CNNs, Dilated CNNs, FTBs, ...
						3. **solution:** Frequency Transformation Blocks
							 - Employing Fully-Connected (FC) Layers to capture Freq-to-Freq Dependencies
							 - (empirical results) Injecting FCs, called FTBs,  into a Fully 2-D Conv U-Net significantly improves SDR performance
						
						--

						### 1.1. Review: Source Separation

						- separates signals of the specific source from a given mixed-signal
						- Music Source Separation, Speech Enhancement, ...
							<center>
							<img src="https://source-separation.github.io/tutorial/_images/source_separation_io.png" width=30%/>
							</center>
						- Benchmarks
							- [MUSDB18](https://paperswithcode.com/dataset/musdb18)
							- [DNS Challenge (Deep Noise Suppression Challenge)](https://paperswithcode.com/dataset/deep-noise-suppression-2020)
							- [FUSS(Free Universal Sound Separation)](https://paperswithcode.com/dataset/fuss)
						
						--
							 
						### 1.1. Spectrogram-based Source Separation
							
							- Audio Equalizer - Eliminate signals with unwanted frequencies
							![](https://imgur.com/u0AoskM.png)
							
							- Spectrogram-based Source Separation
							1. Apply Short-Time Fourier Transform (STFT) on a mixture waveform to obtain the input spectrograms.
							2. Estimate the vocal spectrograms based on these inputs 
							3. Restore the vocal waveform with inverse STFT (iSTFT).
							
						-- 
							 
						### 1.1. Review: U-Net For Spectrogram-based Separation
						
						- Naive Assumption
							- Assuimg a spectrogram is a two (left and right) - channeled image
							- Spectrogram-based Source Separation can be viewed as an Image-to-Image Translation
							
						<center>
						<img src="https://camo.githubusercontent.com/e2ca5fce45aafa29442a625b94f8987fbfeba61e624daeb3febc24a678772ea9/68747470733a2f2f706963342e7a68696d672e636f6d2f76322d38646638636164316466343765346134626537363533373831353636333335325f31323030783530302e6a7067" width=40%/>
						</center>

						--
							 
						### 1.1. Review: U-Net For Spectrogram-based Separation (2)
						
						- ..., and it works...!
						- Jansson, A., et al. "Singing voice separation with deep U-Net convolutional networks." 18th International Society for Music Information Retrieval Conference. 2017.
						- Takahashi, Naoya, and Yuki Mitsufuji. "Multi-scale multi-band densenets for audio source separation." 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2017.

						--

						### 1.1. Review: U-Net For Spectrogram-based Separation (3)
						
						- Recall the assumption of this approach:
							- Assuming a spectrogram is a two (left and right) - channeled image
							- Spectrogram-based Source Separation $\approx$ Image-to-Image Translation
							- (empirical results) **Fully 2-D Convs** can provide promising results

						- Reality Check: Spectrogram $\neq$ Image
							
						--
						
						### 1.2. Motivation: Spectrogram $\neq$ Image
						
						- Image
							<img src="https://i.imgur.com/oTZdjY5.png" width=50%/>
						
						- Spectrogram
							<img src ="https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Spectrogram_of_violin.png/642px-Spectrogram_of_violin.png" width=50%/>
						
						--

						### 1.2. Review: [convolutional](https://mlnotebook.github.io/post/CNN1/)

						![](assets/convSobel.gif)

						--
							 
						### 1.2. Motivation: Spectrogram $\neq$ Image
						
						- [What's wrong with CNNs and spectrograms for audio processing?](https://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd)
						- The axes of spectrograms do not carry the same meaning
							- ***spatial invariance*** that 2D CNNs provide might not perform as well
						- The spectral properties of sounds are non-local
							- Periodic sounds are typically comprised of a fundamental frequency and a number of **harmonics** which are spaced apart by relationships dictated by the source of the sound. It is the mixture of these harmonics that determines the timbre of the sound.
						
						<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Spectrogram_of_violin.png/642px-Spectrogram_of_violin.png" width=40%/>
						
						--
							 
						### 1.2. Motivation: Harmonics and Timbre
						
						![](https://imgur.com/Bsdv5VI.png) ![](https://imgur.com/wM3k1iL.gif)
					
						Harmonics 
						
						<img src="https://imgur.com/zG32WnQ.png" width=30%/>
						
						Timbre of 'Singing Voice' - decided by resonance patterns
						
						--
						
						### 1.3. Proposed Method: FTB
						
						- FTB: Frequency Transformation Blocks
							- Time-Distributed Fully-Connected Layer ([TDF](https://lightsaft.github.io/slide/gaudio/#17))

						<img src="https://github.com/ws-choi/ISMIR2020_U_Nets_SVS/raw/cc6fb8048da2af3748aece6ae639af51b93c0a84/paper_with_code/img/tdf.png" width=70%/>

						- Building Block TFC-TDF: Densely connected 2-d Conv (TFC) with TDFs
						
						<img src="https://github.com/ws-choi/ISMIR2020_U_Nets_SVS/raw/cc6fb8048da2af3748aece6ae639af51b93c0a84/paper_with_code/img/tfctdf.png" width=70%/>
						
						--

						
						### 1.3. Proposed Method: U-Net with TFC-TDFs

						<center>
						<img src="https://imgur.com/tmhMpqL.png" width=50%/>  
						
						<p>+</p> 
						
						<img src="https://github.com/ws-choi/ISMIR2020_U_Nets_SVS/raw/cc6fb8048da2af3748aece6ae639af51b93c0a84/paper_with_code/img/tfctdf.png" width=50%/>
						</center>
						
						--
						
						### 1.3. Results?
						
						- Ablation (n_fft = 2048)
						- U-Net with 17 TFC blocks: SDR 6.89dB
						- U-Net with 17 TFC-**TDF** blocks: SDR 7.12dB (+0.23 dB)
						
						- Large Model (n_fft = 4096)
						![](https://imgur.com/Dby4Rkw.png)
						
						--
						
						### 1.3. Why does it work?: Weight visualization
						
						- freq patterns of different sources captured by TDFs, of FTBs
						
						<img src="https://imgur.com/bH4cgKq.png) ![width:500](https://imgur.com/8fSULqe.png" width=50% />

						--
						
						<img src="https://imgur.com/C5XUq5W.png" width=70%/>

					</script>

				</section>
				<section data-markdown data-separator="---$" data-separator-vertical="--">
					<script type="text/template">
						### Part 2: Latent Source Analysis for Source Separation

						--

						### Part 2: Latent Source Analysis for Source Separation

						- **review**: Conditioned-U-Net for Conditioned Source Separation
						![width:600](assets/cunet.png)
						- **motivation**: Extending FTB to Conditioned Source Separation
							- Naive Extention: Injecting FTBs into C-U-Net?
							- (emprical results) It works, but ...
						- **solution:** Latent Instrumant Attentive Frequency Transformation 
						- **how to modulate latent features**: more complex manipulation method than FiLM

						--

						### 2.1. Review:  C-U-Net

						- Conditioned-U-Net extends the U-Net by exploiting Feature-wise Linear Modulation (FiLM)

						<img src="https://github.com/gabolsgabs/cunet/raw/master/.markdown_images/overview.png" width=60%/>

						```bibtex
						Meseguer-Brocal, Gabriel, and Geoffroy Peeters. "CONDITIONED-U-NET: INTRODUCING A CONTROL MECHANISM IN THE U-NET FOR MULTIPLE SOURCE SEPARATIONS." Proceedings of the 20th International Society for Music Information Retrieval Conference. 2019.
						```

						--

						#### 2.1. Review: C-U-Net using Feature-wise Linear Modulation

						![width:900](https://github.com/gabolsgabs/cunet/raw/master/.markdown_images/c-u-net.png)

						--
						
						### 2.2. Motivation: Naive Extention
						
						![](https://imgur.com/e6giOhG.png)

						TFC vs TFC-TDF: above our expectation

						- Although it does improve SDR performance by capturing common frequency patterns observed across all instruments,
						- Merely injecting an FTB to a CUNet **does not inherit the spirit of FTBs**
						- We propose the Latent Source-Attentive Frequency Transformation (LaSAFT), a novel frequency transformation block that can capture instrument-dependent frequency patterns by exploiting the scaled dot-product attention

						--
						
						### 2.2. Motivation: Latent Source
						- Extending TDF to the Multi-Source Task
						- Naive Extension: MUX-like approach
							- A TDF for each instrument: $\mathcal{I}$ instrument => $\mathcal{I}$ TDFs
							![](https://upload.wikimedia.org/wikipedia/commons/e/e0/Telephony_multiplexer_system.gif)

						- There are much more 'instruments' we have to consider in fact
							- female-classic-soprano, male-jazz-baritone ... $\in$ 'vocals' 
							- kick, snare, rimshot, hat(closed), tom-tom ... $\in$ 'drums'
							- contrabass, walking bass piano (boogie woogie) ... $\in$ 'bass'  
						
						--

						#### 2.3. Latent Source-attentive Frequency Transformation

						- We assume that there are  $\mathcal{I}_L$ latent instruemtns
						  - string-finger-low_freq
						  - string-bow-low_freq
						  - brass-high-solo
						  - ...
						- We assume each instrument can be represented as a weighted average of them
						  - bass: 0.7 string-finger-low_freq + 0.2 string-bow-low_freq + 0.1 percussive-low						
						- LaSAFT
						  - $\mathcal{I}_L$ TDFs for  $\mathcal{I}_L$ latent instruemtns
						  - attention-based weighted average
						
						--
												
						### 2.3. LaSAFT: Extending TDF to the Multi-Source Task 

						- Conceptual View of LaSAFT
						- Suppose there are $\mathcal{I}$ instruments in the dataset
						- We use $\mathcal{I}_{L}$ TDFs
							- , where $\mathcal{I}_{L}>\mathcal{I}$
						- For condition vector $C$, we attentively aggregate the TDFS' results.

						- Effects of employing LaSAFTs instead of TFC-TDFs

						![width:600](https://imgur.com/sPVDDzZ.png)

						--

						#### 2.4. GPoCM: more complex manipulation than FiLM
						
						<img src="https://imgur.com/A3kAxVS.png" width=50%/>

						FiLM: Feature-wise Linear Modulation

						<img src="https://imgur.com/9A4otVA.png" width=50%/>
						
						PoCM: Point-wise Convolutional Modulation

						--
						#### 2.4. GPoCM: more complex manipulation than FiLM

						- PoCM is an extension of FiLM. 
						  - while FiLM does not have inter-channel operations
						  - PoCM has inter-channel operations

						  <img src="https://imgur.com/9A4otVA.png" width=50%/>

						--
						
						## Experimental Results

						![width:1200](https://imgur.com/GYaO0Aa.png)

						--

						## LaSAFT + GPoCM

						- achieved [state-of-the-art](https://paperswithcode.com/sota/music-source-separation-on-musdb18?p=lasaft-latent-source-attentive-frequency) SDR performance on vocals and other tasks in Musdb18.

						![width:900](https://imgur.com/A7JpmD9.png)
						news: outdated :(
					</script>
				</section>
	
			<section data-markdown data-separator="---$" data-separator-vertical="--">
				<script type="text/template">
					### Part 3. Audio Manipulation with Textual Queries
					- AMSS 
						- Audio Manipulation on User-Specified Sources
					- AML
						- Audio Manipulation Language
					
					--

					### 3.1. AMSS					
					- AMSS is formally defined as follows:
						- for the given audio track $A$ and a description $S$,
						- the AMSS aims to generate a manipulated audio track $A'$ that semantically matches $S$
						- while preserving contents in $A$ that are irrelevant to $S$,
						- where $A$ contains signals from multiple sources and $S$ describes how to manipulate audio signals from specific sources in $A$.

					- A source separation task can be viewed as an AMSS task where we want to simply all mute the unwanted sources.

					--

					### 3.2. Scope of the research
					
					- As a proof-of-concept, this paper aims to verify that it is possible to train a deep learning model for AMSS.
					- we focus on modifying specific sources' sonic characteristics (e.g., loudness, panning, frequency content, and dereverberation).
					
					--

					### 3.3. Audio Manipulation Language

					- Audio Manipulation Language is proposed based on a probabilistic Context-Free Grammar (CFG)
					- [Full Grammar](https://kuielab.github.io/AMSS-Net/aml.html)
					- Query Examples
						```python
						model.manipulate_track(audio, 'decrease the volume of bass') 
						model.manipulate_track(audio, 'pan vocals completely to the left side') 
						model.manipulate_track(audio, 'apply heavy lowpass to drums, vocals') 
						model.manipulate_track(audio, 'apply medium highpass to vocals, drums') # == apply highpass to drums, vocals 
						model.manipulate_track(audio, 'separate vocals, bass, drums') # == extract vocals, drums, bass
						model.manipulate_track(audio, 'mute bass, drums')  # == get rid of drums, bass
						model.manipulate_track(audio, 'remove reverb from drums, bass') 
						```
					
					--

					### 3.4. How to train a model for AMSS?

					- Supervised AMSS training
						- Provided a dataset of triples $\{(A^{(i)}, A'^{(i)}, S^{(i)})\}_{i=1}^{N}$, 
							- we can train $net$, a neural network, for AMSS in a supervised manner by minimizing $\sum_{i=1}^{N}loss(net(A^{(i)}, S^{(i)}), A'^{(i)})$, where $loss$ is a distance metric such as $L_{2}$.
						
						- However, there were no datasets available that directly address AMSS currently.
							- For AMSS training, we propose a training framework that uses a multitrack dataset such as Musdb18 \cite{musdb18}. 
							- By applying Digital Signal Processing (DSP) to target sources of a given multitrack audio file, we can generate an AMSS triple on-the-fly.

					--
					#### 3.4. AMSS triple generators for Supervised AMSS Training

					- $\sum_{i=1}^{N}loss(net(A^{(i)}, S^{(i)}), A'^{(i)})$,
					  - $A^{(i)}$: audio before manipulation
					  - $A'^{(i)}$: audio after manipulation
					  - $S^{(i)}$: description
					
					- An example of AMSS triple generator based on DSP
					  ![width:900](assets/training_framework.png)
					
					  --

					  #### 3.5 Another challenge: Sound objects are `transparent'
					  
					  - ManiGAN cannot directly address AMSS becuase audio and image are different.
					  - Unlike images, sound objects are `transparent';
						- a pixel in an image generally corresponds to a single visual object, 
						- whereas a sound object (e.g., a sample in a wave, a frequency bin in a spectrogram) does not only carry information of a single source.
					  
						![](assets/challenge.png)
					  
					  --
					  
					  ### 3.6. AMSS-Net

					  --

					  ### 3.6. Conceptual View of AMSS-Net
					  
					  ![](assets/conceptual.png)
					  
					  - extract ***latent sources*** from a mixture into latent source
					  - and **selectively manipulates** the feature map, while preserving irrelevant latent sources.
					  
					  --
					  
					  #### 3.6. Latent Sources
					  
					  Proposed for source separation by existing works.
					  
					  -  Wisdom, Scott, et al. "Unsupervised Sound Separation Using Mixture Invariant Training." (2020). Neurips
						  > The model is trained to separate this input into a variable number of latent sources, such that the separated sources can be remixed to approximate the original mixtures.
					  
					  - Woosung Choi et al. LaSAFT
						- There are much more 'instruments' we have to consider in fact
					  female-classic-soprano, male-jazz-baritone ... ∈ 'vocals'
					  kick, snare, rimshot, hat(closed), tom-tom ... ∈ 'drums'
					  contrabass, electronic, walking bass piano (boogie woogie) ... ∈ 'bass'
					  
					  
					  --
					  
					  ## AMSS-Net
					  
					  - Overview: A conditioned Dense U-Net structure 
						- similar to ***MMDenseNet*** with a conditioning mechanism
						![](assets/architecture.png) 
					  
					  - Decoder
						![](assets/decoder.png)
					  
					  --
					  
					  ## Decoding Block
					  
					  ![](assets/decoder.png)
					  
					  - Input
						- $X_{D}^{K}$: features from the previous decoding block
						- $X_{E}^{K}$: features from the skip connection
						- $w\in \mathbb{R}^{L\times E}$: word features
					  --
					  
					  ## Decoding Block: Latent Source Extraction
					  
					  ![](assets/decoder.png)
					  
					  - Internel features
						- $V^{ch}\in\mathbb{R}^{M\times T \times F}$: extracted features of latent sources
						  - We assume that each latent source is isolated in a separate channel
					  
					  --
					  
					  ## Decoding Block: Selective Manipulation based on three PoCMs
					  
					  ![](assets/decoder.png)
					  
					  - Internel features
						- $V^{ch}\in\mathbb{R}^{M\times T \times F}$: extracted features of latent sources
						  - We assume that each latent source is isolated in a separate channel
						- $V'^{ch} \in\mathbb{R}^{M\times T \times F}$: selectively manipulated features
						  - if $i^{th}$ latent source should be preserved for the given input, then the $i^{th}$ channel of $V'^{ch}$ would be trained to have near zero values.
					  
					  --
					  
					  ## PoCM (from previous work)
					  
					  - FiLM (left) vs PoCM (right)
					  
					  ![width:550](https://imgur.com/A3kAxVS.png) ![width:550](https://imgur.com/9A4otVA.png)
					  
					  - PoCM is an extension of FiLM
						- while FiLM does not have inter-channel operations
						- PoCM has inter-channel operations
					  
					  - PoCM can be viewed as a channel-level affine transformation
						- easy to implement and computationally cheep!
						
					  --
					  
					  ## PoCM (from previous work)
					  
					  - PoCM is an extension of FiLM
					  
						- $FiLM(X^{i}_{c}|\gamma_{c}^{i},\beta_{c}^{i}) =  \gamma_{c}^{i} \cdot X^{i}_{c} + \beta_{c}^{i}$
					  
						- $PoCM(X^{i}_{c}|\omega_{c}^{i},\beta_{c}^{i}) = \beta_{c}^{i} + \sum_{j}{\omega_{cj}^{i} \cdot X^{i}_{j}}$
						  - where $\gamma_{c}^{i}$ and $\beta_{c}^{i}$ are parameters generated by the condition generator, and $X^{i}$ is the output of the $i^{th}$ decoder's intermediate block, whose subscript refers to the $c^{th}$ channel of $X$
					  
						  ![width:500](https://imgur.com/9A4otVA.png)
					  
					  --
					  
					  ## Decoding Block: SMPoCM
					  
					  - (SMPoCM) Selective Manipulation by Point-wise Convolutional Modulation
					  
						- $SMPoCM(X|\theta) = (1-s) \odot X + g \odot tanh(PoCM(s \odot X, \theta_{m}))$
						  - , where
							- $s=PoCM(X, \theta_{s})$: selective gate
							- $g=PoCM(X, \theta_{g})$: input gate
							- $tanh(PoCM(s \odot X, \theta_{m}))$: manipulation
					  
					  --
					  
					  ## Decoding Block: Channel-wise Skip Attention
					  
					  ![](assets/decoder.png)
					  
					  - The goal of CSA is to minimize information loss during channel reconstruction to preserve other features that are irrelevant to the description.
					  
					  - The attention weight $CSA(Q^{ch}_{t},K^{ch}_{t})_{i,j}$ represents the correlation between the $i^{th}$ channel of the original audio features and the $j^{th}$ latent source channel of the decoded audio features.
					   
					  --
					  
					  ## Decoding Block: Channel-wise Skip Attention (2)
					  
					  ![](assets/decoder.png)
					  
					  - $Q^{ch}$: query features from the skip connection
						- it is ***not manipulated***
					  - $K^{ch}$: key features after latent source extraction
						- it is ***not manipulated***
					  - $V'^{ch} \in\mathbb{R}^{M\times T \times F}$: selectively **manipulated**
					  
					  --
					  
					  ## Experimental Result
					  
					  ![](assets/exp.png)

				</script>
			</section>




			</div>
		</div>
		<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				math: {
					mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
					config: 'TeX-AMS_HTML-full',
					// pass other options into `MathJax.Hub.Config()`
					TeX: { Macros: { RR: "{\\bf R}" } }
					},
				hash: true,
				slideNumber: true,
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
			});
		</script>
	</body>
</html>
