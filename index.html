<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Woosung Choi - Dissertation</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section data-markdown data-separator="---$" data-separator-vertical="--">
					<script type="text/template">
						## Deep Learning-based Latent Source Analysis for Source-aware Audio Manipulation

						Woosung Choi (ws_choi@korea.ac.kr)
		
						--
		
						#### Deep Learning-based
						#### Latent Source Analysis 
						#### for Source-aware Audio Manipulation
		
						Woosung Choi (ws_choi@korea.ac.kr)
		
						Ph.D. Candidate, Department of Computer Science
		
						College of Informatics, Korea University
		
						June 01, 2021
		
						--
		
						Supervisor: Prof. Soonyoung Jung

						--

						## Opening BGM
						<audio controls src="assets/www_vocals.mp3" data-autoplay>Opening bgm</audio>

						--
						## Audio Manipulation on Specified Sources!
						
						- What a wonderful world - Louis Armstrong
						
						<iframe width="560" height="315" src="https://www.youtube.com/embed/p42esUHqq8Q?start=16" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

						```python
						model.manipulate_track(audio.T, "remove vocals") 
						```
					</script>
				</section>

				<section data-markdown data-separator="---$" data-separator-vertical="--">
					<script type="text/template">
						## Contents
						- Introduction

						- Part 1: FTB for Source Separaton
						- Part 2: Latent Source Analysis for Source Separation
						- Part 3: Beyond Source Separation: AMSS!

						- Discussion and Conclusion

						---

						### Introduction

						- It is difficult for non-experts to edit multimedia content such as image, audio, and video.

						![](assets/image_editing.jpg)

						--

						### Object Removal with Photoshop

						![https://digitalsynopsis.com/design/photoshop-remove-unwanted-objects-content-aware-fill/](assets/photoshop.jpg)

						https://digitalsynopsis.com/design/photoshop-remove-unwanted-objects-content-aware-fill/

						--

						### Image Inpainting - Nvidia

						- When deep learning meets image editing

						![](assets/nvidia.gif) 

						https://www.nvidia.com/en-us/research/ai-playground/

						--
						
						### Image Inpainting - Nvidia

						- When deep learning meets image editing

						![](assets/nvidia2.gif)

						https://www.nvidia.com/en-us/research/ai-playground/
										
						--
						
						### Describe What To Change!

						<img src=assets/dwtc.png width=70%/>

						Liu, Yahui, et al. "**Describe What to Change**: A Text-guided Unsupervised Image-to-Image Translation Approach." Proceedings of the 28th ACM International Conference on Multimedia. 2020.

						--

						### Meanwhile, Machine Learning for Audio ...

						--

						### Audio Manipulation is more challenging 

						![](assets/challenge.png)

						- A sound object (i.e., a waveform sample or frequency bin) is `transparent'
						- It usually carries information from multiple sources, in contrast to a pixel in an image

						--

						### The goal of this dissertation
						<p class="fragment" data-fragment-index="2">To design a neural network </p> 
						<p class="fragment" data-fragment-index="3">that performs audio transformations to user-specified sources (e.g., vocals) of a given audio track </p>
						<p class="fragment" data-fragment-index="4">according to a given description while preserving other sources not mentioned in the description</p>
						<br>
						<p class="fragment" data-fragment-index="5">, also known as AMSS!</p>
					
						--
						<center>
						<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Audio source separation is boring. AMSS is the new cool thing now! Next step I guess is stg like Siri integration into Logic, &quot;Siri, a bit more warmth to vocals, and make the guitar tone more brown. Please.&quot; <a href="https://twitter.com/hashtag/sourceseparation?src=hash&amp;ref_src=twsrc%5Etfw">#sourceseparation</a> <a href="https://twitter.com/hashtag/audioseparation?src=hash&amp;ref_src=twsrc%5Etfw">#audioseparation</a> <a href="https://t.co/VJcKT3JtY7">https://t.co/VJcKT3JtY7</a></p>&mdash; András Barják (@Forevian) <a href="https://twitter.com/Forevian/status/1391492024583561218?ref_src=twsrc%5Etfw">May 9, 2021</a></blockquote> 
						</center>
						--

						### Terminologies
						<p class="fragment" data-fragment-index="2"><b>audio signal</b>: a series of sampled values</p> 
						<p class="fragment" data-fragment-index="3"><b>frequency</b>: the number of occurrences of a repeating event per unit of time </p>
						<p class="fragment" data-fragment-index="4"><b>timbre</b>: perceived sound quality of a musical note, </p>
						<p class="fragment" data-fragment-index="5">, which makes a particular musical instrument or human voice have a different sound from anothe </p>

					</script>

				</section>

				<section data-markdown data-separator="---$" data-separator-vertical="--">
					<script type="text/template">
						### Part 1: FTB for Source Separation

						--

						### Part 1: FTB for Source Separation
						
						1. **review**: a U-Net for Spectrogram-based Source Separation
						2. **motivation**: Spectrogram $\neq$ Image
							 - What's wrong with CNNs and spectrograms for audio processing?
							 - Alternatives: 1-D CNNs, Dilated CNNs, FTBs, ...
						3. **solution:** Frequency Transformation Blocks
							 - Employing Fully-Connected (FC) Layers to capture Freq-to-Freq Dependencies
							 - (empirical results) Injecting FCs, called FTBs,  into a Fully 2-D Conv U-Net significantly improves SDR performance
						
						--

						### 1.1. Review: Source Separation

						- separates signals of the specific source from a given mixed-signal
						- Music Source Separation, Speech Enhancement, ...
							<center>
							<img src="https://source-separation.github.io/tutorial/_images/source_separation_io.png" width=30%/>
							</center>
						- Benchmarks
							- [MUSDB18](https://paperswithcode.com/dataset/musdb18)
							- [DNS Challenge (Deep Noise Suppression Challenge)](https://paperswithcode.com/dataset/deep-noise-suppression-2020)
							- [FUSS(Free Universal Sound Separation)](https://paperswithcode.com/dataset/fuss)
						
						--
							 
						### 1.1. Spectrogram-based Source Separation
							
							- Audio Equalizer - Eliminate signals with unwanted frequencies
							![](https://imgur.com/u0AoskM.png)
							
							- Spectrogram-based Source Separation
							1. Apply Short-Time Fourier Transform (STFT) on a mixture waveform to obtain the input spectrograms.
							2. Estimate the vocal spectrograms based on these inputs 
							3. Restore the vocal waveform with inverse STFT (iSTFT).
							
						-- 
							 
						### 1.1. Review: U-Net For Spectrogram-based Separation
						
						- Naive Assumption
							- Assuimg a spectrogram is a two (left and right) - channeled image
							- Spectrogram-based Source Separation can be viewed as an Image-to-Image Translation
							
						<center>
						<img src="https://camo.githubusercontent.com/e2ca5fce45aafa29442a625b94f8987fbfeba61e624daeb3febc24a678772ea9/68747470733a2f2f706963342e7a68696d672e636f6d2f76322d38646638636164316466343765346134626537363533373831353636333335325f31323030783530302e6a7067" width=40%/>
						</center>

						--
							 
						### 1.1. Review: U-Net For Spectrogram-based Separation (2)
						
						- ..., and it works...!
						- Jansson, A., et al. "Singing voice separation with deep U-Net convolutional networks." 18th International Society for Music Information Retrieval Conference. 2017.
						- Takahashi, Naoya, and Yuki Mitsufuji. "Multi-scale multi-band densenets for audio source separation." 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2017.

						--

						### 1.1. Review: U-Net For Spectrogram-based Separation (3)
						
						- Recall the assumption of this approach:
							- Assuming a spectrogram is a two (left and right) - channeled image
							- Spectrogram-based Source Separation $\approx$ Image-to-Image Translation
							- (empirical results) **Fully 2-D Convs** can provide promising results

						- Reality Check: Spectrogram $\neq$ Image
							
						--
						
						### 1.2. Motivation: Spectrogram $\neq$ Image
						
						- Image
							<img src="https://i.imgur.com/oTZdjY5.png" width=50%/>
						
						- Spectrogram
							<img src ="https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Spectrogram_of_violin.png/642px-Spectrogram_of_violin.png" width=50%/>
						
						--

						### 1.2. Review: [convolutional](https://mlnotebook.github.io/post/CNN1/)

						![](assets/convSobel.gif)

						--
							 
						### 1.2. Motivation: Spectrogram $\neq$ Image
						
						- [What's wrong with CNNs and spectrograms for audio processing?](https://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd)
						- The axes of spectrograms do not carry the same meaning
							- ***spatial invariance*** that 2D CNNs provide might not perform as well
						- The spectral properties of sounds are non-local
							- Periodic sounds are typically comprised of a fundamental frequency and a number of **harmonics** which are spaced apart by relationships dictated by the source of the sound. It is the mixture of these harmonics that determines the timbre of the sound.
						
						<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Spectrogram_of_violin.png/642px-Spectrogram_of_violin.png" width=40%/>
						
						--
							 
						### 1.2. Motivation: Harmonics and Timbre
						
						![](https://imgur.com/Bsdv5VI.png) ![](https://imgur.com/wM3k1iL.gif)
					
						Harmonics 
						
						<img src="https://imgur.com/zG32WnQ.png" width=30%/>
						
						Timbre of 'Singing Voice' - decided by resonance patterns
						
						--
						
						### 1.3. Proposed Method: FTB
						
						- FTB: Frequency Transformation Blocks
							- Time-Distributed Fully-Connected Layer ([TDF](https://lightsaft.github.io/slide/gaudio/#17))

						<img src="https://github.com/ws-choi/ISMIR2020_U_Nets_SVS/raw/cc6fb8048da2af3748aece6ae639af51b93c0a84/paper_with_code/img/tdf.png" width=70%/>

						- Building Block TFC-TDF: Densely connected 2-d Conv (TFC) with TDFs
						
						<img src="https://github.com/ws-choi/ISMIR2020_U_Nets_SVS/raw/cc6fb8048da2af3748aece6ae639af51b93c0a84/paper_with_code/img/tfctdf.png" width=70%/>
						
						--

						
						### 1.3. Proposed Method: U-Net with TFC-TDFs

						<center>
						<img src="https://imgur.com/tmhMpqL.png" width=50%/>  
						
						<p>+</p> 
						
						<img src="https://github.com/ws-choi/ISMIR2020_U_Nets_SVS/raw/cc6fb8048da2af3748aece6ae639af51b93c0a84/paper_with_code/img/tfctdf.png" width=50%/>
						</center>
						
						--
						
						### 1.3. Results?
						
						- Ablation (n_fft = 2048)
						- U-Net with 17 TFC blocks: SDR 6.89dB
						- U-Net with 17 TFC-**TDF** blocks: SDR 7.12dB (+0.23 dB)
						
						- Large Model (n_fft = 4096)
						![](https://imgur.com/Dby4Rkw.png)
						
						--
						
						### 1.3. Why does it work?: Weight visualization
						
						- freq patterns of different sources captured by TDFs, of FTBs
						
						<img src="https://imgur.com/bH4cgKq.png) ![width:500](https://imgur.com/8fSULqe.png" width=50% />

						--
						
						<img src="https://imgur.com/C5XUq5W.png" width=70%/>

					</script>

				</section>
				<section data-markdown data-separator="---$" data-separator-vertical="--">
					<script type="text/template">
						### Part 2: Latent Source Analysis for Source Separation

						--

						### Part 2: Latent Source Analysis for Source Separation

						- **review**: Conditioned-U-Net for Conditioned Source Separation
						![width:600](assets/cunet.png)
						- **motivation**: Extending FTB to Conditioned Source Separation
							- Naive Extention: Injecting FTBs into C-U-Net?
							- (emprical results) It works, but ...
						- **solution:** Latent Instrumant Attentive Frequency Transformation 
						- **how to modulate latent features**: more complex manipulation method than FiLM

						--

						### 2.1. Review:  C-U-Net

						- Conditioned-U-Net extends the U-Net by exploiting Feature-wise Linear Modulation (FiLM)

						<img src="https://github.com/gabolsgabs/cunet/raw/master/.markdown_images/overview.png" width=60%/>

						```bibtex
						Meseguer-Brocal, Gabriel, and Geoffroy Peeters. "CONDITIONED-U-NET: INTRODUCING A CONTROL MECHANISM IN THE U-NET FOR MULTIPLE SOURCE SEPARATIONS." Proceedings of the 20th International Society for Music Information Retrieval Conference. 2019.
						```

						--

						#### 2.1. Review: C-U-Net using Feature-wise Linear Modulation

						![width:900](https://github.com/gabolsgabs/cunet/raw/master/.markdown_images/c-u-net.png)

						--
						
						### 2.2. Motivation: Naive Extention
						
						![](https://imgur.com/e6giOhG.png)

						TFC vs TFC-TDF: above our expectation

						- Although it does improve SDR performance by capturing common frequency patterns observed across all instruments,
						- Merely injecting an FTB to a CUNet **does not inherit the spirit of FTBs**
						- We propose the Latent Source-Attentive Frequency Transformation (LaSAFT), a novel frequency transformation block that can capture instrument-dependent frequency patterns by exploiting the scaled dot-product attention

						--
						
						### 2.2. Motivation: Latent Source
						- Extending TDF to the Multi-Source Task
						- Naive Extension: MUX-like approach
							- A TDF for each instrument: $\mathcal{I}$ instrument => $\mathcal{I}$ TDFs
							![](https://upload.wikimedia.org/wikipedia/commons/e/e0/Telephony_multiplexer_system.gif)

						- There are much more 'instruments' we have to consider in fact
							- female-classic-soprano, male-jazz-baritone ... $\in$ 'vocals' 
							- kick, snare, rimshot, hat(closed), tom-tom ... $\in$ 'drums'
							- contrabass, walking bass piano (boogie woogie) ... $\in$ 'bass'  
						
						--

						#### 2.3. Latent Source-attentive Frequency Transformation

						- We assume that there are  $\mathcal{I}_L$ latent instruemtns
						  - string-finger-low_freq
						  - string-bow-low_freq
						  - brass-high-solo
						  - ...
						- We assume each instrument can be represented as a weighted average of them
						  - bass: 0.7 string-finger-low_freq + 0.2 string-bow-low_freq + 0.1 percussive-low						
						- LaSAFT
						  - $\mathcal{I}_L$ TDFs for  $\mathcal{I}_L$ latent instruemtns
						  - attention-based weighted average
						
						--
												
						### 2.3. LaSAFT: Extending TDF to the Multi-Source Task 

						- Conceptual View of LaSAFT
						- Suppose there are $\mathcal{I}$ instruments in the dataset
						- We use $\mathcal{I}_{L}$ TDFs
							- , where $\mathcal{I}_{L}>\mathcal{I}$
						- For condition vector $C$, we attentively aggregate the TDFS' results.

						- Effects of employing LaSAFTs instead of TFC-TDFs

						![width:600](https://imgur.com/sPVDDzZ.png)

						--

						### 2.4. GPoCM: more complex manipulation method than FiLM
						
						- FiLM (left) vs PoCM (right)
						
						![width:550](https://imgur.com/A3kAxVS.png) ![width:550](https://imgur.com/9A4otVA.png)
						
						- PoCM is an extension of FiLM. 
						  - while FiLM does not have inter-channel operations
						  - PoCM has inter-channel operations

						--
						
						## Experimental Results

						![width:1200](https://imgur.com/GYaO0Aa.png)

						--

						## LaSAFT + GPoCM

						- achieved [state-of-the-art](https://paperswithcode.com/sota/music-source-separation-on-musdb18?p=lasaft-latent-source-attentive-frequency) SDR performance on vocals and other tasks in Musdb18.

						![width:900](https://imgur.com/A7JpmD9.png)
						news: outdated :(
					</script>
				</section>
	
			<section data-markdown data-separator="---$" data-separator-vertical="--">
				<script type="text/template">
					### 3. Audio Manipulation with Textual Queries
					- AMSS: Audio Manipulation on User-Specified Sources with Textual Queries
					  - Submitted to ACMMM 2021
					  - demo: http://intelligence.korea.ac.kr/demo/
						```python
						model.manipulate_track(audio, 'decrease the volume of bass') 
						model.manipulate_track(audio, 'pan vocals completely to the left side') 
						model.manipulate_track(audio, 'apply heavy lowpass to drums, vocals') 
						model.manipulate_track(audio, 'apply medium highpass to vocals, drums') # == apply highpass to drums, vocals 
						model.manipulate_track(audio, 'separate vocals, bass, drums') # == extract vocals, drums, bass
						model.manipulate_track(audio, 'mute bass, drums')  # == get rid of drums, bass
						model.manipulate_track(audio, 'remove reverb from drums, bass') 
						```	

				</script>
			</section>




			</div>
		</div>
		<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				math: {
					mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
					config: 'TeX-AMS_HTML-full',
					// pass other options into `MathJax.Hub.Config()`
					TeX: { Macros: { RR: "{\\bf R}" } }
					},
				hash: true,
				slideNumber: true,
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
			});
		</script>
	</body>
</html>
